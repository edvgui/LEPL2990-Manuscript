\chapter{Background knowledge}

In this chapter I will put some basis, and explain some concepts related to my work.  Reading this chapter should allow the reader to understand what I did in this work, regardless of its background.

\section{Virtualisation, Containerisation, Runtime isolation}
\paragraph{}We always want the same thing, be able to execute some code, with no interaction with someone else's one.  As of course we can not use one bare-metal machine for each application we want to run, some mechanism have to be used to provide isolation between coexisting processes on a same physical machine.

We can distinguish three level of isolation: Virtualisation, Containerisation and Runtime isolation (Figure \ref{fig:virt-cont-runt}).
\begin{figure}
  \begin{center}
    \includegraphics[width=\linewidth]{images/Virt-Cont-Runt.pdf}
    \caption{Different isolation level for an application.}
    \label{fig:virt-cont-runt}
  \end{center}
\end{figure}
\subsection{Virtualisation}
The isolation layer is located either between the hardware and the guest OS (hypervisor of type 1) or between a virtualisation of the hardware, by an hypervisor (type 2) running in another OS, and the guest OS.  In both cases, the guest OS, will run as if it was on a bare-metal machine, executing its own kernel, managing its drivers, its file system, etc.  This is the stronger isolation we can get.  Cloud providers can rent you some Virtual Machines, this is what we call IaaS (Infrastructure as a Service).  You can use many different hypervisor solutions, usually, Cloud providers have their very own solution.
\subsection{Containerisation}
Here we go one level higher, the isolation layer is between the OS and the guest application.  This isolation mechanism is called a container, and is basically a set of processes, running in common namespaces, with a root filesystem different from the host.  Containers share the same kernel as the host though, which means that a process running in a container can be seen from the host.  Containers belong to the world of PaaS (Platform as a Service), a system where Cloud providers offers you to run your application (that you provide under a containerized form) whithout having to worry about all the infrastructure than needs to be deploied along with it, and sometimes even with some great scalability mechanism support.
\subsection{Runtime isolation}
This is the highest level of (weakest) isolation you can get, the isolation is provided by the runtime on which the application is running (ex: The JVM for a Java program, a Python environment, ...).  Multiple guest applications share the same OS, file system, whithout any mechanism to hide it from them.  Cloud providers propose such service under the name of FaaS (Function as a Service).  For this you can provide a small program, that you want to be executed on demand.  Cloud providers usually allocate a container to this program, to provide a safer isolation, but will execute multiple instances of this program in the same containers, to avoid the overhead of creating a new containers each time.  The kinds of programs you usually run in these are computionnaly intensive parts of your application, that needs to have low response time and great scalability (for example resizing of user uploaded images).  This model is often reference as the Lambda model (from Amazon Lambda service) or as serverless computing.

\subsection{INGInious use case}
The case of INGInious is a special one.  The type of the workload caused by the tasks evaluation (fast response time, varying demand, potentially big computation, logic independent of the core application) would suggest to use a serverless strategy, but the isolation requirement are those of PaaS (as the various inputs of the functions are code of student to safely execute, isolated from one another).  And for now the all application relies on IaaS, running in multiple VMs where are hosted the core application and multiple docker agents.  

\section{Containers}
The isolation being the most important requirement, we will then look into containerisation solutions.  Presenting the main concepts behind containerisation, the global container ecosystem, the current solution that INGInious uses and the alternatives we have.

\subsection{Core concepts}
Containers rely on three components: namespaces, control groups and chroot.
\paragraph{}\textbf{Namespaces} are a feature of the Linux kernel since 2002, they are a key element that made containerisation possible.  A namespace can be associated to a context, which is a partition of all the resources of a system that a set of processes has access to, while other processes can't.  Those sets of resources can be of seven kinds:
\begin{itemize}
\renewcommand\labelitemi{--}
  \item \textbf{mnt} (Mount): This controls the mount points.  Processes can only have access to the mount point of their namespace.
  \item \textbf{pid} (Process ID): This allows each process in each namespace to get a process id assigned indepedently of other namespaces porcesses.
  \item \textbf{net} (Network): This provides a virtualized network stack.
  \item \textbf{ipc} (Interprocess Communication): This allows processes of a same namespace to communicate with one another, for example by sharing some memory.
  \item \textbf{uts} (Hostname): This allows to have different hostnames on a same machine, each hostname being considered as unique by the processes of its namespace.
  \item \textbf{user} (User ID): This allows to change the user id in a namespace.
  \item \textbf{cgroup} (Control group): This allows to change the root cgroup directory, this virtualize how process's cgroups are viewed.
\end{itemize}

\paragraph{}When a Linux machine starts, it initiate one namespace of each type in which all the processes run.  The processes can then choose to create and switch of namespaces.

\paragraph{}\textbf{Control groups} are a feature of the Linux kernel that allows to limit and control the resources allocated to some processes.  You can for example control the cpu usage, the memory consumption, the io...  Recently (since kernel 4.5) a new version of cgroups (cgroups v2) appeared, which comes to tackle the flaws of the original implementation, while keeping all of its functionalities.  Though, the adoption of the new version is a process, and takes times, and still now many applications use the original version of cgroup (like runc).

\paragraph{}\textbf{Chroot} allows to change the root of the root filesystem for some processes.  For example, we could create a directory \texttt{/tmp/myroot/} which contains all of the usual directories present in the original root folder (\texttt{/}) and set this as the new apparent root for the chosen processes.  This is not a complete sandbox, and not a real isolation on its own, files from outside of the chroot could still be accessed.

\subsection{Storage drivers}
When it comes to handle a container file system, different solutions can be used.  The goal of each is to provide the most efficient writable root directory (\texttt{/}) for each container, but keeping each container inaffected by the modifification done in the other containers.

\paragraph{}In order to do so, three main strategies can be used:
\begin{itemize}
\renewcommand\labelitemi{--}
  \item \textbf{Deep copy}: for each container, the whole image is copied during the creation of the container.  This is simple, but gets terribly slow as the file system size increases.
  \item \textbf{File based copy on write}: for each container, will be copied only the files that are edited during the container life cycle.  This is more complex, but get more efficient as the container's size grows.
  \item \textbf{Block based copy on write}: for each container, will be copied only the blocks (in the filesystem) that are edited during the container life cycle.  This is even more complex, but get more efficient as some small part of big files are edited.
\end{itemize}

\paragraph{}Containers are a specific kind of workload in the sense that many information, data, is redundant in different containers.  For example, for a simple application, we could use several containers with different responsabilities and tools embedded in it, but all based on the same Alpine image.  This brought a new space problem, as we don't want to avoid duplicating too much data.  In order to face this, \textbf{union filesystems} are used, along with layered container images.  This basically means that different container images but with the same basis, will actually share this same basis, avoiding the need to duplicate it.

\paragraph{}Here is a non-exhaustive list of current available solutions for container storage:
\begin{itemize}
\renewcommand\labelitemi{--}
  \item \texttt{overlay} and \textbf{overlay2} (\textit{Docker}): Those are based on OverlayFS (Linux kernel driver), a union filesystem, similar to AUFS, but Docker claims it to be faster and simpler.  Overlay2 is the new and more stable version of overlay.
  \item \texttt{aufs} (\textit{Docker}): This solution is based on AUFS, a union filesystem.  For Docker, it was the predecessor of overlay, and is a bit less performant than the latest.
  \item \texttt{btrfs} (\textit{Docker}, \textit{LXC}):  Btrfs is a copy-on-write filesystem.  Docker's and LXC's btrfs storage driver rely directly on it, using its block-level capabilities, thin provisionning and its copy-on-write snapshots.
  \item \texttt{zfs} (\textit{Docker}, \textit{LXC}): ZFS is another filesystem, with many features, like snapshots, compression, deduplication and more.  Docker's and LXC's zfs storage driver rely on it, taking advantages of its capabilities.
  \item \texttt{vfs} (\textit{Docker}):  This is the simplest and yet more reliable storage driver.  As it doesn't provide any advanced feature.  It as no copy-on-write capabilities.  Each container filesystem is compied on creation.  The performances of this driver are very poor then.
  \item \texttt{directory} (\textit{LXC}):  This is the exact equivalent of vfs.
  \item \texttt{devicemapper} (\textit{Docker}) [deprecated]:  This relies on Device Mapper, a kernel-base framework with some interesting capabilities as snapshots and thin provisionning.  This is a block-based approach.
  \item \texttt{lvm} (\textit{LXC}):  This is the exact equivalent of devicemapper.
\end{itemize}

\textbf{Note} that the \texttt{storage} Go library for containers, wrapped under the \texttt{containers-storage} CLI\footnote{\href{https://github.com/containers/storage}{https://github.com/containers/storage}} provide support for all of those drivers\footnote{\href{https://github.com/containers/storage/tree/master/drivers}{https://github.com/containers/storage/tree/master/drivers}}.

\subsection{Container runtime}
A container runtime is a tool that creates containers, executes process in it, and deletes dead containers.  It will have the responsability to create the namespaces, change the root directory, and attach processes to a control group.  The most common container runtime nowadays is \texttt{runc}, created by the Open Container project.

\subsection{Container manager}
A container manager will use the container runtime, to provide a "user-friendly" interface to manage containers.  It has the responsability to set up the network interfaces, to provide the image of the containers and any Copy-on-write mechanism that could go along with it.  It creates and manages the control groups in with containers will be set.  Some of them offer the possibility to create custom images, to create pods, or swarm, which are entities of multiple interconnected containers.

\subsection{Ecosystem overview}
A small overview of the current container ecosystem can be found on Figure \ref{fig:overview}.  Note that solutions like Kubernetes\footnote{Kubernetes is "an open-source system for automating deployment, scaling, and management of containerized applications" \cite{kubernetes}} which are more oriented towards hosting and continuous deployment of container based applications than to single container provisionning are not presented here\footnote{A more detailed overview dor those kind of applications is provided by Containerd at the following address: \href{https://containerd.io/img/architecture.png}{https://containerd.io/img/architecture.png}}.
\begin{figure}[h!]
  \begin{center}
    \includegraphics[width=\linewidth]{images/ecosystem.pdf}
    \caption{Small overview of the current container ecosystem}
    \label{fig:overview}
  \end{center}
\end{figure}
\paragraph{}\textbf{OCI} (Open Container Initiative) is "an open governance structure for the express purpose of creating open industry standards around container formats and runtime."\cite{oci} They currently have two specifications: the format of the image that has to be given to an OCI compatible runtime, and the commands to interact with such runtime.

\subsection{Containerisation solutions}
\paragraph{}\textbf{Docker}\cite{merkel2014docker} is today probably the most known solution for containerization.  Since its apparition in 2014, its interest for the PaaS sector hasn't ceased to grow.  It consists in a daemon, running on the host, that allows to easily manage different containers.  It relies on Containerd, "An industry-standard container runtime with an emphasis on simplicity, robustness and portability"\cite{containerd}, which is a more complete container runtime than what I presented before, with embedded network management, storage driver management, and other mechanism.  Containerd is not meant to be used standalone though, which is why we still use Docker.  By default, Docker uses \texttt{runc} as container runtime, but is compatible with any runtime that fulfill the OCI\cite{oci} requirements.

The main advantages of Docker are its simplicity of use, its production-grade quality, the huge fleet of ready-to-run containers publicly available and a lot of useful tools (like \texttt{docker-swarm} that come along with it).  This is the solution currently used by INGInious.

\paragraph{}"\textbf{Podman} is a daemonless container engine for developing, managing, and running OCI\cite{oci} Containers on your Linux System."\cite{podman}  Podman presents itself as a viable true alternative to Docker.  Its main difference is the fact that it runs daemonless, it runs containers as detached child processes.  It can also manage pods, which are groups of containers deployed on the same machine. Its default runtime is \texttt{runc} as well.  Podman is an Open-Source project, and is still growing a lot, the latest version at this day (2020-02-28) is v1.8.0, released 21 days ago, and already 287 commits have been done since then!

\paragraph{}"\textbf{LXC} is a userspace interface for the Linux kernel containment features."\cite{lxc}  This solution is developed and maintained by Canonial Ltd.  Docker used to be based on LXC until it created its own execution environment.  LXC came out recently with a new solution; LXD, which offer about the same things as Docker does.  A bunch of ready to run images publicly available, and a nice command line interface to interact with containers.  The only missing feature of LXC compare to Docker, regadering our use case, is the possibility to launch a container with a command, and stop it when it is finished.  LXC actually start a complete init process for each container, in which you can then come and execute your command.

\paragraph{}\textbf{Kata Containers} is not another container manager.  It is an OCI compatible runtime.  With the specificity that it doesn't run mainstream containers as \texttt{runc}, but actually lightweight virtual machines, with Qemu (originaly) or Firecracker as hypervisor.
They put forward four features of their solution:
\begin{itemize}
\renewcommand\labelitemi{--}
  \item \textbf{Security}: Thanks to their virtualization solution, each runtime has its own kernel, with real network, i/o and memory isolation.
  \item \textbf{Compatibility}:  They support industry standarts, as the OCI\cite{oci} and legacy virtualization technologies.
  \item \textbf{Performance}:  Their performances are consistent with classical containerization solutions.
  \item \textbf{Simplicity}:  They eliminate the need to have a virtual machine dedicated to host containers.
\end{itemize}

\paragraph{}\textbf{crun} is a complete equivalent of \texttt{runc}, yet another OCI compatible runtime, but this one is implemented in C, which give it a small performance advantage over \texttt{runc}, is implemented in GO.  \texttt{crun} has also full support for cgroupv2, which is still lacking for \texttt{runc} at the moment (2020-04-22).
