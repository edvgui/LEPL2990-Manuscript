\chapter{Related work}

This is not the first time that someone tries to compare different containerization solutions.  In this chapter I will present other work that has been done for that matter, and what I can add up to that.  I will end up by presenting some improving solutions that some people tryied to bring to the container solution panel.

\section{Performance studies}

\begin{description}
  \item[2013]\textit{Performance evaluation of container-based virtualization for high performance computing environments}\cite{xavier2013performance}: They compared a native experience to LXC containers, OpenVZ, VServer and Xen.  They showed that virtualisation (with Xen) was giving really poor performances compared to the others (which give nearly-native experience), in terms of I/O, memory bandwidth, and even network latency and bandwidth.  What they have to grant to virtualization though, is the better isolation it provides, as an application running in a VM could run almost seemlessly when the system is stressed (in another VM) with varying tests.  While for other solution the impact can vary a lot, even making it impossible for the application to start in some case.
  
  \item[2014] \textit{Virtualization vs containerization to support paas}\cite{dua2014virtualization}: As said previously, Virtual Machines have been kept away from the PaaS world, because of the greater overhead that Virtualization has on booting time and resources consumptions of the isolated system created.  This paper made a detailed point about it, emphasizing on three points that needed to be improved for the next generation of containers: Security, OS Independence and Standardization.  The last one meaning that containers runtime should establish some common requirements, come up with a common container format.  We have seen this appear since then, under the OCI (Open Container Initiative) specifications.  A thradeoff with the performances (mostly I/O) of the application running isolated is much more evident with virtualisation than with containerisation.
  
  \item[2015] \textit{An updated performance comparison of virtual machines and linux containers}\cite{felter2015updated}:  In this paper they show the overhead that both containerization and virtualization have, compared to a bare-metal system.  The most important point is the I/O comparison, where they show that virtualization (here with kvm) provide much lower performances than containerization (with docker, using aufs).  And docker performances can even be improved to nearly meet the native ones when using volumes to store the solicitated files.
  
  \item[2017] \textit{Performance overhead comparison between hypervisor and container based virtualization}\cite{li2017performance}:  In this paper they showed that even though container-based virtualization (with Docker) is more lightweight than real virtualisation, its performances are not always better (they are actually worse when trying to read or write single bytes to disk).  And they plan to compare it into more details later.
  
  \item[2017] \textit{A performance comparison of container-based technologies for the cloud}\cite{kozhirbayev2017performance}: They compared here two different containerization solutions (Docker and LXC) against the native experience with focus on CPU usage, memory consumption, network and I/O performances.  Once again, overheads are detected for I/O operations, where high demand of input-output operations have a greater latency that a native experience would have.
\end{description}

\section{Improvement studies}

\subsection{SAND} 
\paragraph{}SAND\cite{akkus2018sand} aims at improving the performance of serverless computing with two techniques: application-level sandboxing, and a hierarchical message bus:

\paragraph{}\textbf{Application Sandboxing}: The key idea here is to differentiate multiple executions of different functions with multiple execution of a same function.  In the first case, as usual, a new container is launched on the new function demand and the function is executed inside of it.  In the second case, instead of lauching a new container with exactly the same configuration as an already running one, we only fork a process inside de running container to deal with the new demand, which is much faster than creating a new container.  The problem in our case, is that we loose the isolation between student code execution, which is not desirable.
\paragraph{}\textbf{Hierarchical Message Queuing}: The goal to this is to facilitate communication between functions that interact with one another (i.e. the output of a function is the input of another one).  To do so they use a two-level communication bus: global and local.  Global means that the communication is made between different functions on different hosts, while local means different functions on the same host.  As accessing the local bus is much faster than the global one, it can decrease latency for functions on the same host.  For now in the case of INGInious, student and teacher container always run on the same host and communicate though the mean of file descriptors.  So this is still not something for us.  % In our case, as INGInious is still running on a single host, this shouldn't improve our performances that much.  But in a near future where INGInious get a new horizaontally scaled architecture, it might be interesting to come back on it.


\subsection{SOCK} \label{subsec-sock}
\paragraph{}"Sock (roughly for serverless-optimized containers), [is] a special-purpose container system with two goals: (1) \textit{low-latency invocation} for Python handlers that import librairies and (2) \textit{efficient sandbox initialization} so that individual workers can achive high ready-state throughput." \cite{oakes2018sock}  The final product created here isn't really something we could use for INGInious, it is a serverless solution (based on the Lambda model), targeting specifically python applications.  Though, in the process of creating this solution, they started from containers and deconstructed their perfromances, indentifying their bottlenecks.  And from this we can take those things:
\begin{itemize}
\renewcommand\labelitemi{--}
  \item Bind mounting is twice as fast as AUFS.  Even though AUFS (used by Docker) as a usefull Copy-on-write capability, we don't need to write to most of our files in our case, so using a read-only bind mounting for those files could allow us to avoid copying all file before startup, while still beeing able to edit the one we want to.
  \item Network namespaces creation and cleanup are costly, due to a single lock shared across all network namespaces.  We might gain some performances buy not adding network interfaces to containers that don't need it.  This is actually already done by the docker agent of INGInious.
  \item Reusing a cgroup is at least twice as fast as creating a new one each time.  We could keep a pool of initialized cgroups, and only change the current container it controls.
\end{itemize}

\subsection{LightVM} 
\paragraph{}LigthVM is a complete redesign of Xen's toolstack which tried to bring some container's characteristics to VMs.  Such as fast instantiation (small startup time) and high instance density capability (high number of instances running in parallel on the same machine).\cite{manco2017my}  Xen is a Type-1\footnote{A Type-1 hypervisor is an hypervisor that runs on a bare-metal machine, whithout additional host.} hypervisor presented in 2003 with really low virtualization overheads and high hosting capacity, allowing a machine to host up to a 100 guest OS.\cite{barham2003xen}  
\paragraph{}They achieve such container's like performances by:
\begin{itemize}
\renewcommand\labelitemi{--}
  \item reducing the image size and the memory footprint of virtual machines.  They do so by including in the VM only what is necessary to the application that is meant to be executed in it.
  \item introducing noxs (no XenStore), a new implementation of Xen, without XenStore (which was a real bottleneck for fast instantiation of multiple VMs).
  \item splitting the Xen's toolstack into what can be run before the VM creation and what as to be done during it.  Allowing to pre-initialize VMs.
  \item replacing the Hotplug Script by xendevd, a binary deamon that can execute pre-defined setup more efficiently.
\end{itemize}

\paragraph{}They present four use cases with this solution, one of them beeing more intreresting for us: lightweight computation services, for which they rely on Minipython unikernel, to run computations written in Python, as a Faas could propose to do.  This would still be hard to use for INGInious, for the same reason as for SOCK (ยง\ref{subsec-sock}).

\subsection{Firecracker} 
\paragraph{} Firecracker is a new VMM (hypervisor) created specifically for serverless and containers applications.  \cite{agachefirecracker}  This is a solution provided by AWS, that very recently got deployed for two of their web services: Lambda (Faas) and Fargate (Paas).  We are basically getting here the good isolation of virtual machines and nearly as good performances and low overhead of containers.  Firecracker is based on KVM and provides minimal virtual machines (MicroVMs).  The configuration is done through a REST API.  Device emulation is available for disks, networking and serial console.  Network can be limited and so can disk throughput and request rate.  If proven to be easily usable in INGInious case, this would provide a better alternative to classical containerisation regarding security.

\subsection{Summary}
\paragraph{}In this section, on Table \ref{tab:summary}, are quickly reminded the several new solutions we explored in this chapter, along with some interesting infos about them.
\begin{table}[!h]

  \begin{center}
    \begin{tabular}{|p{.2\textwidth}|p{.1\textwidth}|p{.1\textwidth}|p{.1\textwidth}|p{.1\textwidth}|p{.1\textwidth}|p{.1\textwidth}|}
       \hline
       \textbf{Name} & \textbf{Pub.} & \textbf{Update} & \textbf{Open-Source} & \textbf{Com.} & \textbf{Isol.} \\
       \hline
       SAND & 2018 & ? & ? & No & Cont. \\
       \hline
       SOCK & 2018 & ? & ? & No & Cont. or Runt. \\
       \hline
       LigthVM & 2017 & 2017 & Yes & No & Virt. \\
       \hline
       Firecracker & 2019 & 2020 & Yes & Yes & Virt.\\
       \hline
    \end{tabular}
  \end{center}
  \caption{Summary table of the different solutions explored in this chapter.}
  \label{tab:summary}
\end{table}
\paragraph{}\textbf{Caption}: 
\begin{itemize}
\renewcommand\labelitemi{--}
  \item \textit{Name}: The name of the project.
  \item \textit{Pub.}: The first publication year of the project.
  \item \textit{Update}: The last update year of the project.
  \item \textit{Open-source}: If the project is open-source.
  \item \textit{Com.}: If the project is a "commercial grade" solution.
  \item \textit{Isol.}: The type of isolation used in the project.
  \item \textit{Cont.}: Isolation by containerization.
  \item \textit{Virt.}: Isolation by virtualization.
  \item \textit{Runt.}: Isolation by the runtime of the application.
\end{itemize}

\section{My master thesis}
In this chapter I presented an overview of the work of other people in the field of containerization, with a focus on performance improvement or measurement.  Unfortunetly, we have more things to test, and more questions to ask than what those papers had to offer.

First, most of those performance measurement are quite old, and none of them considered all of the solutions that we could use for INGInious case.  For example Podman is never mentionned once (which is normal as version 1.0.0 of Podman was only released in January 2019).  Neither is the interesting case of Kata-container.  The apparition of lightweight hypervisor like Firecracker is a game changer, but the paper didn't provide any performance comparision with containerization solutions.

Second, none of the research here had a real focus on the time required for each solution to provide a ready to run environnement.  We have got a lot a CPU and memory overhead analysis, but those are not our main concerns.

This is the direction I will take in this report.  Trying to respond to the question: "What would be the most appropriate containerisation (or virtualisation) solution for INGInious use case?", or in other word, how to get the shortest instantiation time, along with the best theoretical isolation possible and as low overall performance overhead as we can get.
