\chapter{Related work}

This is not the first time that researchers try to compare different containerization solutions.  In this chapter, I will present other works that have been done in that matter and show how what I am doing is different.  I will end up by presenting some improving solutions that some people tried to bring to the container solution panel.

\section{Performance studies}

\begin{description}
  \item[2013]\textit{Performance evaluation of container-based virtualization for high-performance computing environments}\cite{xavier2013performance}: They compared a native experience to LXC containers, OpenVZ, VServer and Xen.  They showed that virtualization (with Xen) was giving really poor performances compared to the others (which give nearly-native experience), in terms of I/O, memory bandwidth, and even network latency and bandwidth.  What they have to grant to virtualization though, is the better isolation it provides, as an application running in a VM could run almost seamlessly when the system is stressed (in another VM) with varying tests.  While for other solutions the impact can vary a lot, even making it impossible for the application to start in some case.
  
  \item[2014] \textit{Virtualization vs containerization to support paas}\cite{dua2014virtualization}: Virtual Machines have been kept away from the PaaS world, because of the greater overhead that Virtualization has on booting time and resources consumptions of the isolated system created.  This paper made a detailed point about it, emphasizing three points that needed to be improved for the next generation of containers: Security, OS Independence and Standardization.  The last one meaning that containers runtime should establish some common requirements, come up with a common container format.  We have seen this appear since then, under the OCI (Open Container Initiative) specifications.  They also showed that a trade-off with the performances (mostly I/O) of the application running isolated is much more evident with virtualization than with containerization.
  
  \item[2015] \textit{An updated performance comparison of virtual machines and Linux containers}\cite{felter2015updated}:  In this paper, they show the overhead that both containerization and virtualization have, compared to a bare-metal system.  The most important point is the I/O comparison, where they show that virtualization (here with KVM) provides much lower performances than containerization (with docker, using aufs).  And docker performances can even be improved to nearly meet the native ones when using volumes to store the solicited files.
  
  \item[2017] \textit{Performance overhead comparison between hypervisor and container based virtualization}\cite{li2017performance}:  In this paper, they showed that even though container-based virtualization (with Docker) is more lightweight than real virtualization, its performances are not always better (they are worse when trying to read or write single bytes to disk).  And they plan to compare it into more details later.
  
  \item[2017] \textit{A performance comparison of container-based technologies for the cloud}\cite{kozhirbayev2017performance}: They compared here two different containerization solutions (Docker and LXC) against the native experience with a focus on CPU usage, memory consumption, network and I/O performances.  Once again, overheads are detected for I/O operations, where high demand for input-output operations have a greater latency that a native experience would have.
\end{description}

\section{Improvement studies}

\subsection{SAND} 
\paragraph{}SAND\cite{akkus2018sand} aims at improving the performance of serverless computing with two techniques: application-level sandboxing, and a hierarchical message bus:

\paragraph{}\textbf{Application Sandboxing}: The key idea here is to differentiate multiple executions of different functions with multiple executions of the same function.  In the first case, as usual, a new container is launched on the new function demand and the function is executed inside of it.  In the second case, instead of launching a new container with the same configuration as an already running one, they only fork a process inside the running container to deal with the new demand, which is much faster than creating a new container.  This would not be good for us as we would lose the isolation between student code execution, which is not desirable.
\paragraph{}\textbf{Hierarchical Message Queuing}: The goal to this is to facilitate communication between functions that interact with one another (i.e. the output of a function is the input of another one).  To do so they use a two-level communication bus: global and local.  Global means that communication is made between different functions on different hosts, while local means different functions on the same host.  As accessing the local bus is much faster than the global one, it can decrease latency for functions on the same host.  In the case of INGInious, student and teacher containers always run on the same host and communicate though the mean of file descriptors.  Therefore, this is not something for us.


\subsection{SOCK} \label{subsec-sock}
\paragraph{}"Sock (roughly for serverless-optimized containers), [is] a special-purpose container system with two goals: (1) \textit{low-latency invocation} for Python handlers that import libraries and (2) \textit{efficient sandbox initialization} so that individual workers can achieve high ready-state throughput." \cite{oakes2018sock}  The final product created here is not something we could use for INGInious, it is a serverless solution (based on the Lambda model), targeting specifically python applications.  Though, in the process of creating this solution, they started from containers and deconstructed their performances, identifying their bottlenecks.  And from this we can :
\begin{itemize}
\renewcommand\labelitemi{--}
  \item Bind mounting is twice as fast as AUFS.  Even though AUFS (used by Docker) has a useful copy-on-write capability, we do not need to write to most of our files in our case, so using a read-only bind mounting for those files could allow us to avoid copying all file before start-up, while still being able to edit the files we want to, avoiding the cost of copying the file to a higher layer.
  \item Network namespaces creation and clean-up are costly, due to a single lock shared across all network namespaces.  We might gain some performances buy not adding network interfaces to containers that do not need it.  This is already done by the docker agent of INGInious.
  \item Reusing a cgroup is at least twice as fast as creating a new one each time.  We could keep a pool of initialized cgroups, and only change the current container it controls.
\end{itemize}

\subsection{LightVM} 
\paragraph{}LigthVM is a complete redesign of Xen's toolstack which tried to bring some container's characteristics to VMs.  Such as fast instantiation (small start-up time) and high instance density capability (high number of instances running in parallel on the same machine).\cite{manco2017my}  Xen is a Type-1\footnote{A Type-1 hypervisor is a hypervisor that runs on a bare-metal machine, without an additional host.} hypervisor presented in 2003 with really low virtualization overheads and high hosting capacity, allowing a machine to host up to a 100 guest OS.\cite{barham2003xen}  
\paragraph{}LightVM achieves such container's like performances by:
\begin{itemize}
\renewcommand\labelitemi{--}
  \item reducing the image size and the memory footprint of virtual machines.  They do so by including in the VM only what is necessary to the application that is meant to be executed in it.
  \item introducing noxs (no XenStore\footnote{The XenStore is a registry containing informations about running VMs and their devices.}), a new implementation of Xen, without XenStore (which was a real bottleneck for fast instantiation of multiple VMs).
  \item splitting the Xen's toolstack into what can be run before the VM creation and what as to be done during it.  Allowing to pre-initialize VMs.
  \item replacing the Hotplug Script by xendevd, a binary daemon that can execute pre-defined setup more efficiently.
\end{itemize}

\paragraph{} The paper presents four use cases with this solution, one of them being more interesting for us: lightweight computation services, for which they rely on Minipython unikernel, to run computations written in Python, as a Faas could propose to do.  This would still be hard to use for INGInious, for the same reason as for SOCK (ยง\ref{subsec-sock}).

\subsection{Firecracker} 
\paragraph{} Firecracker is a new VMM (hypervisor) created specifically for serverless and container applications.  \cite{agachefirecracker}  This is a solution provided by AWS, that very recently got deployed for two of their web services: Lambda (Faas) and Fargate (Paas).  We are getting here the good isolation of virtual machines and nearly as good performances and low overhead of containers.  Firecracker is based on KVM and provides minimal virtual machines (MicroVMs).  The configuration is done through a REST API.  Device emulation is available for disks, networking and serial console.  The network can be limited and so can disk throughput and request rate.  If proven to be easily usable in INGInious's case, this would provide a better alternative to classical containerization regarding security.

\subsection{Summary}
\paragraph{}In this section, on Table \ref{tab:summary}, are quickly reminded the several new solutions we explored in this chapter.
\begin{table}[!h]

  \begin{center}
    \begin{tabular}{|p{.2\textwidth}|p{.1\textwidth}|p{.1\textwidth}|p{.1\textwidth}|p{.1\textwidth}|p{.1\textwidth}|p{.1\textwidth}|}
       \hline
       \textbf{Name} & \textbf{Pub.}\footnotemark & \textbf{Update}\footnotemark & \textbf{Open-Source} & \textbf{Com.}\footnotemark & \textbf{Isol.}\footnotemark \\
       \hline
       SAND\cite{akkus2018sand} & 2018 & ? & ? & No & Cont. \\
       \hline
       SOCK\cite{oakes2018sock} & 2018 & ? & ? & No & Cont. or Runt. \\
       \hline
       LigthVM\cite{manco2017my} & 2017 & 2017 & Yes & No & Virt. \\
       \hline
       Firecracker\cite{agachefirecracker} & 2019 & 2020 & Yes & Yes & Virt.\\
       \hline
    \end{tabular}
  \end{center}
  \caption{Summary table of the different solutions explored in this chapter.}
  \label{tab:summary}
\end{table}
\footnotetext[1]{The year of the publication of the project}
\footnotetext[2]{The year of the last update on the project}
\footnotetext[3]{Whether the solution has a "commercial grade" quality}
\footnotetext[4]{The type of isolation of the solution; cont. for containerization, virt. for virtualization, runt. for runtime isolation}

\section{My master thesis}
In this chapter, I have presented an overview of the work of other people in the field of containerization, with a focus on performance improvement or measurement.  Nevertheless, these solutions cannot be used without testing and some more questions are rising which were not answered in the consulted literature.

First of all, most of those performance measurements are quite old, and none of them considered all of the solutions that we could use for INGInious's case.  For example, Podman is not mentioned once (which is normal as version 1.0.0 of Podman was only released in January 2019).  Neither is the interesting case of Kata Containers.  The apparition of a lightweight hypervisor like Firecracker is a game-changer, but the paper did not provide any performance comparison with containerization solutions.

Secondly, none of the research here had a real focus on the time required for each solution to provide a ready to run environment.  We have got a lot a CPU and memory overhead analysis, but those are not our main concerns.

Therefore, the direction I will take in this report is trying to answer to the question: "What would be the most appropriate containerization (or virtualization) solution for INGInious use case?", or in other words, how to get the shortest instantiation time, along with the best theoretical isolation possible and as low overall performance overhead as we can get.
