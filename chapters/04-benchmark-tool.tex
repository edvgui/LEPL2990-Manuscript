\chapter{Benchmark tool}
In this chapter I will present the setup used to perform my measurements, the differents tests I made and the candidates solutions that are going to be compared with one another.

\section{Setup}
\subsection{Testing environment}

\paragraph{}Originally, all of my tests where meant to be executed in a Virtual Machine, on my computer.  In a VM because this is currently the nature of the environnement where INGInious launches its container.  And on my computer, because it is  powerful enough for the modest load of tests that I am running for each solution.  Unfortunetly, the first setup I had didn't support nested virtualisation (which is required for Kata-based solutions), the second one was very unstable with it and I finally moved to the cloud, renting Azure VMs.  

The early results that I could get where quite surprising though, showing really poor performances for Kata containers with Firecracker, and it turned out that Firecracker is not really meant to be running inside VMs (it can work, but not at its best).  I made then a last move, to an Intel NUC provided by the INGI department.  All the tests will then run on a bare-metal machine, no more nested virtualization problem at the horizon.

The testing environment used for the final results presented in this work is an Intel NUC with the following configuration:\\
\begin{tabular}{rl}

  \textbf{processor} & Intel(R) Core(TM) i7-7567U CPU @ 3.50GHz \\
  \textbf{memory} & 16GB \\
  \textbf{main partition} & 2TB HDD \\
  \textbf{additionnal storage} & 500GB SSD \\
  \textbf{operation system} & Ubuntu 18.04.4 LTS \\
  \textbf{linux kernel version} & 4.15.0-99-generic \\

\end{tabular}
\paragraph{}\textbf{Note about storage:} The main partition, on which the operating system is running, is a hard drive.  This will provide much lower IO performances than an SSD.  To be more in pair with the kind of performances we can get in the cloud, all the relevant data accessed and written during my tests will be stored on the additionnal disk (SSD).

\subsection{Configuration of the environment}
As a lot of different solutions are going to be tested, and all of them with their own requirements (sometimes conflicting with the ones of other solutions), some work has to be done to make it easier for someone to replicate them.  The configuration of the environnment is going to be done with Ansible\footnote{https://docs.ansible.com/} playbooks, with one playbook for each solution to test.

All the different configurations allong with the tests they are required for and their corresponding playbook are presented in section \ref{subs:candidates}.

\section{Tests}
As we alreay said previously, time is an important aspect in the experience INGInious provides to its users.  The time for a task to be evaluated and the time before this task is evaluated.  In order to improve those two, we will focus in those tests on the booting time of containers and their IO handling (still related to the time overhead that some solution could have compared to another).  % TODO IO big files and IO lots of file

While we are at it, we will also verify if some solutions handle network much poorly than others.  Each test done is presented in more details in section \ref{subs:experiments}.


\subsection{Candidates}\label{subs:candidates}
A candidate solution is a combinaison of different elements, variabilities, choices to make.  We will consider here those ones:
\begin{itemize}
  \renewcommand\labelitemi{--}
  \item \textbf{Container manager}: \textit{Docker}, \textit{Podman}, \textit{LXD}, those are the different user friendly solution that can be used to manage container, and that we will compare here.
  \item \textbf{Container runtime}: \textit{runc}, \textit{crun}, \textit{LXC}, \textit{Kata containers} (Qemu or Firecracker), the (less user-friendly) container runtimes, taking care of all the isolation that a container require.
  \item \textbf{Control group}: \textit{cgroup} and \textit{cgroupv2}, there is no real choice to make here, cgroupv2 is the successor of cgroup, and should be used, when supported by the other element of the configuration.
  \item \textbf{Storage driver}: \textit{overlay}, \textit{aufs}, \textit{btrfs}, \textit{zfs}, \textit{vfs}, \textit{lvm}, those are the different strategies that can be used to manage the file system of the container.
  \item \textbf{Base container image}: \textit{Alpine} because it is the default choice when it comes to conceive containerized applications today and \textit{Centos} because it is the current choice made by INGInious.
\end{itemize}

Though, we can not compose a candidate solution with one element of each category, randomly picked.  Some element of the solution might only be usable with some of the possibilities for a variability.  The different constraints that we have are listed here:
\begin{itemize}
  \renewcommand\labelitemi{--}
  \item Docker doesn't support cgroupv2 yet (actually the support has to be added by containerd).
  \item Podman doesn't support cgroup (v1).
  \item LXD doesn't support cgroupv2 yet.
  \item LXD only supports LXC as runtime, and LXC is only supported by LXD.
  \item LXD only supports btrfs, zfs, vfs (under the name of dir) and lvm as storage driver.
  \item Kata Container with Firecracker only supports devicemapper (lvm) as storage driver.
\end{itemize}

The list of different candidate solution we can compare is presented in Table \ref{tab:candidates}.

\begin{table}[!h]
  \begin{center}
    \begin{tabular}{|r|c|c|c|c|c|}
      \hline
      \textbf{\#} & \textbf{Cont. Mngr} & \textbf{Base Image} & \textbf{Storage driver} & \textbf{Ctrl grp} & \textbf{Runtime} \\ \hline \hline
       1  & Docker & Alpine & overlay2 & cgroup & runc \\ \hline \hline
       2  & Docker & Alpine & \textbf{aufs} & cgroup & runc \\ \hline
       3  & Docker & Alpine & \textbf{devicemapper} & cgroup & runc \\ \hline
       4  & Docker & Alpine & \textbf{btrfs} & cgroup & runc \\ \hline
       5  & Docker & Alpine & \textbf{zfs} & cgroup & runc \\ \hline
       6  & Docker & Alpine & \textbf{vfs} & cgroup & runc \\ \hline
       7  & Docker & \textbf{Centos} & overlay2 & cgroup & runc \\ \hline
       8  & Docker & Alpine & overlay2 & cgroup & \textbf{kata-qemu} \\ \hline
       9  & Docker & Alpine & \textbf{devicemapper} & cgroup & \textbf{kata-qemu} \\ \hline
       10 & Docker & Alpine & \textbf{devicemapper} & cgroup & \textbf{kata-fc} \\ \hline
       11 & Docker & Alpine & overlay2 & cgroup & \textbf{crun} \\ \hline
       12 & \textbf{Podman} & Alpine & overlay2 & \textbf{-} & runc \\ \hline
       13 & \textbf{Podman} & Alpine & overlay2 & \textbf{cgroupv2} & \textbf{crun} \\ \hline
       14 & \textbf{-} & Alpine & \textbf{-} & cgroup & runc \\ \hline
       15 & \textbf{-} & Alpine & \textbf{-} & \textbf{cgroupv2} & \textbf{crun} \\ \hline
       16 & \textbf{LXD} & Alpine & \textbf{btrfs} & cgroup & \textbf{LXC} \\ \hline
       17 & \textbf{LXD} & Alpine & \textbf{zfs} & cgroup & \textbf{LXC} \\ \hline
       18 & \textbf{LXD} & Alpine & \textbf{directory} & cgroup & \textbf{LXC} \\ \hline
       19 & \textbf{LXD} & Alpine & \textbf{lvm} & cgroup & \textbf{LXC} \\ \hline
    \end{tabular}
  \end{center}
  \caption{}
  \label{tab:candidates}
\end{table}

\subsection{Experiments} \label{subs:experiments}
For all the experiments presented here, time measurements are taken at four important steps of the execution of the container:
\begin{description}
  \item[$t_0$] Before its creation, this is time zero.
  \item[$t_c$] After its creation. Container's state is \textit{created}.
  \item[$t_s$] After its startup. Container's state is \textit{running}.
  \item[$t_e$] After the end of the execution of the command. Container's state is \textit{running}.
\end{description}
This gives us three interesting measurements:
\begin{enumerate}
  \item Creation time: $t_{create}=t_c - t_0$
  \item Starting time: $t_{start}=t_s - t_c$
  \item Execution time: $t_{exec}=t_e - t_s$
\end{enumerate}

In order to do this, each container will be running as first process a shell\footnote{except for LXD, which natively has an init process running in each container}, which will never receive any input, but is here to ensure that the container will stay up as long as we need it.  When we don't need the container anymore, we simply kill all the processes in it.

\subsubsection{Booting time}
This tests aims at determining the candidate solution that can provide a ready to run container the faster.  In order to measure this, a simple container composed of only the base image is created, in which the simple command \texttt{/bin/echo Hello World} is executed.  The best candidate would be the one that has the shortest creation and starting time ($t_{create} + t_{start}$).

\subsubsection{Basic I/O performances}
This tests aims at determining mostly the influence of the usage of each storage driver on the performances of an application that relies highly on it.  Two kind of tests are done here: big file read and write, and great amount of file read and write.  The best candidate solution would be the one to with the shortest creation, starting and execution time ($t_{create} + t_{start} + t_{exec}$).

For the first one, five sqlite databases have been created, of five different size (about one order of magnitude appart from each other).  The read test will be doing a simple \texttt{select * from ...} on each table of the database.  The write test will be adding a copy of each table in the database (\texttt{create table ... as select * from ...}).

For the second one, five loads of small file have been created (10, 100, 1000, 10 000 and 100 000 2KB random files).  The read test will do a \texttt{cat} on each file, and redirect the output to \texttt{/dev/null}.  The write test will extract an archive (\texttt{tar}) containing all those files.

\subsubsection{Network setup time}
This tests aims also at determining the candidate solution that can setup a ready du run container the fastest, but with the added complexity of adding a network interface, and mapped ports, so that the host machine can send requests to a nightweight http server running in the container.  We take one more time measurement here, the time at wich we received the first response at the http requests we send to our server ($t_r$).  The best candidate would be the one that has the sortest delay before this response ($t_{create} + t_{start} + t_{exec} + t_{response}$, with $t_{response}=t_r - t_e$).

\subsubsection{Ping response time}
This really simple test aims at detremining if some solution provide a much weaker network solution to the container than the others.  We simply perform a ping request to \texttt{1.1.1.1}, and compare the response time we get for each solution.  The best solution would be the one with the shortest ping response time.
